| 学号    | 问题 | 困难度 |
| :------| :------ | :------ |
|  1     | 在课程中，我们提到，在特殊情况下，softmax函数是sigmoid函数的一种对等形式，这是为什么呢？ | :fire: :fire: |
|  2     | softmax函数在神经网络中有着很大的作用，你认为一般情况下，softmax函数会被放置在神经网络的哪一层呢？    | :fire:  |
|  3     | 为什么要对label进行one-hot编码？                                              | :fire: |
|  4     |  本章中我们介绍了交叉熵，什么是熵呢？ | :fire: |
|  5     | 多个sigmoid通过叠加也同样可以实现多分类的效果，那么这种叠加和softmax有什么不一样呢？ | :fire: :fire: |
|  6     | 除了交叉熵，还有什么其他的用于神经网络分类问题的损失函数呢？ | :fire: |
|  7     | 熵这个概念贯穿整个机器学习，因为机器学习本身就是一个概率问题，请查找熵的题出者和年份。 | :fire: |
|  8     | 为什么最大化概率与最小化交叉熵是等价的 | :fire: |
|  9     | 在本课中，我们使用了sigmoid函数达成从离散到连续的目标，这里面的sigmoid函数实际叫做激活函数，并且不止这一种，请查找一下其他常用激活函数 | :fire: |
|  10    | 本课中提到的交叉熵是分类问题的损失函数，那么回归问题的损失函数有哪些呢？至少一类 | :fire: |
|  11    | softmax函数很好的解决了多分类问题，但当我们考虑一个分类数目很多的问题时（譬如imagenet，22000个分类），softmax还能有好的效果吗？ | :fire: :fire: |
|  12    | 请尽可能多的列举神经网络常用的结构（例如VGGNet）。 | :fire: :fire: |
|  13    | softmax函数的的作用是什么？（为什么需要它？） | :fire: |