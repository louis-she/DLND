| 学号    | 问题 | 困难度 | 答案  |
| :------| :------ | :------ | :------ |
|  1     | 在课程中，我们提到，在特殊情况下，softmax函数是sigmoid函数的一种对等形式，这是为什么呢？ | :fire: :fire: |      sigmoid函数是softmax函数的一种特殊形式，当处理二分类问题时，softmax函数可以转化为sigmoid函数。具体可以参照http://ufldl.stanford.edu/wiki/index.php/Softmax回归 中 Softmax回归与Logistic 回归的关系 一节的推导  |
|  2     | softmax函数在神经网络中有着很大的作用，你认为一般情况下，softmax函数会被放置在神经网络的哪一层呢？    | :fire:  |    最后的输出层      |
|  3     | 为什么要对label进行one-hot编码？                                              | :fire: |   将非数字变量转化为数字向量  |
|  4     |  本章中我们介绍了交叉熵，什么是熵呢？ | :fire: |                在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA) |
|  5     | 多个sigmoid通过叠加也同样可以实现多分类的效果，那么这种叠加和softmax有什么不一样呢？ | :fire: :fire: |          softmax进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多个sigmoid进行多分类，输出的类别并不是互斥的，即"苹果"这个词语既属于"水果"类也属于"3C"类别。 |
|  6     | 除了交叉熵，还有什么其他的用于神经网络分类问题的损失函数呢？ | :fire: |      L1 loss，L2 loss，regularised expectation loss，Chebyshev loss，hinge loss https://arxiv.org/abs/1702.05659 |
|  7     | 熵这个概念贯穿整个机器学习，因为机器学习本身就是一个概率问题，请查找熵的题出者和年份。 | :fire: |                     在1948年，由克劳德·艾尔伍德·香农将热力学的熵，引入到信息论                            |
|  8     | 为什么最大化概率与最小化交叉熵是等价的 | :fire: |                 因为交叉熵实际是对概率做Log函数后取负号，由于Log函数单调递增，所以他们是等价的           |
|  9     | 在本课中，我们使用了sigmoid函数达成从离散到连续的目标，这里面的sigmoid函数实际叫做激活函数，并且不止这一种，请查找一下其他常用激活函数 | :fire: |    Relu，Tanh, PRelu |
|  10    | 本课中提到的交叉熵是分类问题的损失函数，那么回归问题的损失函数有哪些呢？至少一类 | :fire: |      均方差，smooth L1等                                |
|  11    | softmax函数很好的解决了多分类问题，但当我们考虑一个分类数目很多的问题时（譬如imagenet，22000个分类），softmax还能有好的效果吗？ | :fire: :fire: |      不好，因为数量太多导致softmax的输出概率太小，这种情况可以对每个类别做二分类来解决          |
|  12    | 请尽可能多的列举神经网络常用的结构（例如VGGNet）。 | :fire: :fire: |          VGGNet，ResNet，DenseNet，Inception 等             |
|  13    | softmax函数的的作用是什么？（为什么需要它？） | :fire: |      将神经元的输出转化为概率         |