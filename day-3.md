| 学号    | 问题 | 困难度 |
| :------| :------ | :------ |
|  1     | 这节课的最后，我们比较了梯度下降法与感知器算法的区别，请回忆一下，当分类错误时，这两种算法对边界的处理是什么样的呢？ | :fire: |
|  2     | 对某一点的分类正确时，这一点的梯度是较小还是较大？ | :fire:  |
|  3     |  对某一点的分类错误时，这一点的梯度是较小还是较大？ | :fire: |
|  4     | 从函数图像的几何意义考虑，梯度代表了什么意思呢？ | :fire: |
|  5     | 梯度下降是否一定能找到全局最优解？为什么？ | :fire: |
|  6     | 当我们的误差函数（代价函数）是一个非凸函数时，学习率的大小会对我们寻找全局最优解起到什么样的影响呢？ | :fire: |
|  7     | 在使用梯度下降时，需要进行调优。哪些地方需要调优呢？ | :fire: :fire: |
|  8     | 梯度下降法其实是一个大家族，你能指出我们在课堂中学习的实际是哪一种吗？ | :fire: |
|  9     | 机器学习中的优化算法，除了梯度下降外还有很多，请试着找到他们吧 | :fire: :fire: |
|  10    | 课程中我们学习到了误差函数（课程中是交叉熵）与计算误差函数最小值的方法--梯度下降法，现在我们抛开具体的交叉熵，从更为宏观的角度考察误差函数，那么有一个问题，误差值一定越小越好吗？（注意，此处的误差函数并不特指交叉熵） | :fire: |
|  11    | 梯度下降法其实是一个大家族，我们在课堂中编程使用的实际是批量梯度下降法，请比较一下家族中其他成员与批量梯度下降的区别 | :fire: :fire: |
|  12    | 在梯度下降算法中，有一些常见的参数，请找出来并分析他们的作用 | :fire: :fire: |
|  13    | 梯度下降法其实是一个大家族，我们在课堂中学习的实际是批量梯度下降法，请找出其他的梯度下降法 | :fire: :fire: |